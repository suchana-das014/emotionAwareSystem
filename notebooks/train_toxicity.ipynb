{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Ensure project root is on Python path so `src` can be imported from notebooks\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74a63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.preprocess import clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ac3278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 8)\n",
      "Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# Load the toxicity dataset (limit to 50k rows for faster training/testing)\n",
    "df = pd.read_csv(\"../datasets/toxic/archive (5)/train.csv\", nrows=50000)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f06edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity distribution:\n",
      "0    44845\n",
      "1     5155\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocess: clean text and create binary toxicity label\n",
    "df[\"cleaned\"] = df[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "# Create a binary 'toxic' label (1 if any toxicity flag is set, 0 otherwise)\n",
    "toxicity_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "y = (df[toxicity_cols].sum(axis=1) > 0).astype(int)\n",
    "print(f\"Toxicity distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e140a70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained. Classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and train model\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df[\"cleaned\"])\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "print(f\"Model trained. Classes: {model.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84dd8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toxicity model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "joblib.dump(model, \"../models/toxicity_model.pkl\")\n",
    "joblib.dump(tfidf, \"../models/toxicity_vectorizer.pkl\")\n",
    "print(\"✅ Toxicity model and vectorizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
